{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SVM to detect anomalies in the 15 minutes candles of Eur/Usd over 10 years of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPARK_HOME=C:\\Users\\HP\\Desktop\\Spark\\spark-2.4.5-bin-hadoop2.7\n",
      "env: HADOOP_HOME=C:\\Users\\HP\\Desktop\\Spark\\spark-2.4.5-bin-hadoop2.7\n"
     ]
    }
   ],
   "source": [
    "#Create enviroment variables to find PySpark\n",
    "%env SPARK_HOME=C:\\Users\\HP\\Desktop\\Spark\\spark-2.4.5-bin-hadoop2.7 \n",
    "%env HADOOP_HOME=C:\\Users\\HP\\Desktop\\Spark\\spark-2.4.5-bin-hadoop2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyarrow) (1.15.4)\n",
      "Requirement already satisfied: py4j in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative way to create a Session and a Context\n",
    "#conf = pyspark.SparkConf().setMaster(\"local[*]\").setAppName(\"SVM\").set(\"spark.memory.offHeap.enabled\", \"true\").set(\"spark.memory.offHeap.size\", \"3048576\")\n",
    "#sc = pyspark.SparkContext(conf = conf)\n",
    "#spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-NKMRQ7VB.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a spark session\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different ways of loading a csv file\n",
    "\n",
    "spark_eurusd = spark.read.format('csv').options(header='true', inferSchema='true') \\\n",
    "                .load('C://Users//HP//Desktop//DataScience//AlgoTrading//eurusd15min.csv')\n",
    "\n",
    "#spark_eurusd = spark.read.csv('C://Users//HP//Desktop//DataScience//AlgoTrading//eurusd5min.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different ways of renaming columns\n",
    "\n",
    "#spark_eurusd = spark_eurusd.select(F.col(\"_c0\").alias(\"id\"),\n",
    "#                                   F.col(\"Time\").alias(\"Time\"),\n",
    "#                                   F.col(\"Open\").alias(\"Open\"),\n",
    "#                                   F.col(\"High\").alias(\"High\"),\n",
    "#                                   F.col(\"Low\").alias(\"Low\"),\n",
    "#                                   F.col(\"Close\").alias(\"Close\"))\n",
    "\n",
    "#spark_eurusd = spark_eurusd.toDF(\"id\", \"Time\", \"Open\", \"High\", \"Low\", \"Close\")\n",
    "\n",
    "# A way to reneame only a single column\n",
    "spark_eurusd = spark_eurusd.withColumnRenamed(\"_c0\", \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------+-------+-------+-------+\n",
      "| id|               Time|   Open|   High|    Low|  Close|\n",
      "+---+-------------------+-------+-------+-------+-------+\n",
      "|  0|2010-01-03 17:45:00| 1.4312|1.43172| 1.4312|1.43172|\n",
      "|  1|2010-01-03 18:00:00|1.43172|1.43425|1.43105| 1.4311|\n",
      "|  2|2010-01-03 18:15:00| 1.4315|1.43155| 1.4313|1.43155|\n",
      "|  3|2010-01-03 18:30:00|1.43175| 1.4324|1.43106|1.43106|\n",
      "|  4|2010-01-03 18:45:00|1.43111|1.43157|1.43106|1.43157|\n",
      "+---+-------------------+-------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_eurusd.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_eurusd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To drop columns\n",
    "spark_eurusd = spark_eurusd.drop(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pyspark there is no concept of axis like in pandas (e.g. pd.df.drop(\"column_name\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+-------+-------+----+-----------+----------+\n",
      "|               Time|   Open|   High|    Low|  Close|body|higher_wick|lower_wick|\n",
      "+-------------------+-------+-------+-------+-------+----+-----------+----------+\n",
      "|2010-01-03 17:45:00| 1.4312|1.43172| 1.4312|1.43172| 5.2|        5.2|       5.2|\n",
      "|2010-01-03 18:00:00|1.43172|1.43425|1.43105| 1.4311| 6.2|       25.3|       0.5|\n",
      "|2010-01-03 18:15:00| 1.4315|1.43155| 1.4313|1.43155| 0.5|        0.5|       2.5|\n",
      "|2010-01-03 18:30:00|1.43175| 1.4324|1.43106|1.43106| 6.9|        6.5|       0.0|\n",
      "|2010-01-03 18:45:00|1.43111|1.43157|1.43106|1.43157| 4.6|        4.6|       5.1|\n",
      "+-------------------+-------+-------+-------+-------+----+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column that calculate the absolute value of the body of the 5 minutes candle\n",
    "\n",
    "body = F.udf(lambda x,y: (x-y)*10000, DoubleType())\n",
    "\n",
    "spark_eurusd = spark_eurusd.withColumn(\"body\", F.round(F.abs(body(spark_eurusd[\"Open\"], spark_eurusd[\"Close\"])), 2))\n",
    "spark_eurusd = spark_eurusd.withColumn(\"higher_wick\", F.round(F.abs(body(spark_eurusd[\"High\"], spark_eurusd[\"Open\"])), 2))\n",
    "spark_eurusd = spark_eurusd.withColumn(\"lower_wick\", F.round(F.abs(body(spark_eurusd[\"Close\"], spark_eurusd[\"Low\"])), 2))\n",
    "\n",
    "spark_eurusd.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How to do an aggregation\n",
    "body_aggr = spark_eurusd.groupby([\"body\"]).agg({\"body\": \"count\"}).orderBy(\"body\")\n",
    "body_aggr = body_aggr.withColumnRenamed(\"count(body)\", \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|             body|             count|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|              673|               673|\n",
      "|   mean|36.77265973254087| 386.3298662704309|\n",
      "| stddev|26.44344304300913|1068.2407536192316|\n",
      "|    min|              0.0|                 1|\n",
      "|    max|            223.0|              7538|\n",
      "+-------+-----------------+------------------+\n",
      "\n",
      "+------------------+\n",
      "|   body_percentile|\n",
      "+------------------+\n",
      "|[16.8, 33.6, 50.8]|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistics of body\n",
    "body_aggr.describe().show()\n",
    "body_aggr.selectExpr(\"percentile_approx(body, array(.25, .5, .75)) as body_percentile\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|       higher_wick|\n",
      "+-------+------------------+\n",
      "|  count|            260000|\n",
      "|   mean|4.4696776923077355|\n",
      "| stddev| 5.756607936679121|\n",
      "|    min|               0.0|\n",
      "|    max|             223.1|\n",
      "+-------+------------------+\n",
      "\n",
      "+----------------------+\n",
      "|higher_wick_percentile|\n",
      "+----------------------+\n",
      "|       [1.0, 2.8, 5.8]|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistics of higher_wick\n",
    "spark_eurusd.select(\"higher_wick\").describe().show()\n",
    "spark_eurusd.selectExpr(\"percentile_approx(higher_wick, array(.25, .5, .75)) as higher_wick_percentile\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|       lower_wick|\n",
      "+-------+-----------------+\n",
      "|  count|           260000|\n",
      "|   mean|4.438610769230832|\n",
      "| stddev|5.308289298585862|\n",
      "|    min|              0.0|\n",
      "|    max|            243.7|\n",
      "+-------+-----------------+\n",
      "\n",
      "+---------------------+\n",
      "|lower_wick_percentile|\n",
      "+---------------------+\n",
      "|      [1.2, 2.9, 5.8]|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistics of lower_wick\n",
    "spark_eurusd.select(\"lower_wick\").describe().show()\n",
    "spark_eurusd.selectExpr(\"percentile_approx(lower_wick, array(.25, .5, .75)) as lower_wick_percentile\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols= [\"higher_wick\", \"body\", \"lower_wick\"],\n",
    "    outputCol= \"features\")\n",
    "\n",
    "transformed = assembler.transform(spark_eurusd).withColumn('label', \n",
    "                                                            F.when((F.col(\"body\") > 120) | (F.col(\"body\") < 5) |\n",
    "                                                            (F.col(\"higher_wick\") > 50) | (F.col(\"higher_wick\") < 0.5) | \n",
    "                                                            (F.col(\"lower_wick\") > 50) | (F.col(\"lower_wick\") < 0.5), 1.0)\n",
    "                                                            .otherwise(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- body: double (nullable = true)\n",
      " |-- higher_wick: double (nullable = true)\n",
      " |-- lower_wick: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+-------+-------+----+-----------+----------+--------------+-----+\n",
      "|Time               |Open   |High   |Low    |Close  |body|higher_wick|lower_wick|features      |label|\n",
      "+-------------------+-------+-------+-------+-------+----+-----------+----------+--------------+-----+\n",
      "|2010-01-03 17:45:00|1.4312 |1.43172|1.4312 |1.43172|5.2 |5.2        |5.2       |[5.2,5.2,5.2] |0.0  |\n",
      "|2010-01-03 18:00:00|1.43172|1.43425|1.43105|1.4311 |6.2 |25.3       |0.5       |[25.3,6.2,0.5]|0.0  |\n",
      "|2010-01-03 18:15:00|1.4315 |1.43155|1.4313 |1.43155|0.5 |0.5        |2.5       |[0.5,0.5,2.5] |1.0  |\n",
      "|2010-01-03 18:30:00|1.43175|1.4324 |1.43106|1.43106|6.9 |6.5        |0.0       |[6.5,6.9,0.0] |1.0  |\n",
      "|2010-01-03 18:45:00|1.43111|1.43157|1.43106|1.43157|4.6 |4.6        |5.1       |[4.6,4.6,5.1] |1.0  |\n",
      "|2010-01-03 19:00:00|1.43154|1.43192|1.43136|1.43192|3.8 |3.8        |5.6       |[3.8,3.8,5.6] |1.0  |\n",
      "|2010-01-03 19:15:00|1.43199|1.43212|1.43081|1.43126|7.3 |1.3        |4.5       |[1.3,7.3,4.5] |0.0  |\n",
      "|2010-01-03 19:30:00|1.43115|1.43191|1.43081|1.43191|7.6 |7.6        |11.0      |[7.6,7.6,11.0]|0.0  |\n",
      "|2010-01-03 19:45:00|1.43191|1.43191|1.43106|1.43106|8.5 |0.0        |0.0       |[0.0,8.5,0.0] |1.0  |\n",
      "|2010-01-03 20:00:00|1.43103|1.43201|1.43098|1.43136|3.3 |9.8        |3.8       |[9.8,3.3,3.8] |1.0  |\n",
      "+-------------------+-------+-------+-------+-------+----+-----------+----------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [5.2,5.2,5.2]),\n",
       " LabeledPoint(0.0, [25.3,6.2,0.5]),\n",
       " LabeledPoint(1.0, [0.5,0.5,2.5]),\n",
       " LabeledPoint(1.0, [6.5,6.9,0.0]),\n",
       " LabeledPoint(1.0, [4.6,4.6,5.1])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "labeled_point = transformed.rdd.map(lambda row: LabeledPoint(row['label'], row['features'].toArray()))\n",
    "labeled_point.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "model = SVMWithSGD.train(labeled_point, iterations=100, regType=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.3892884615384615\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on training data\n",
    "\n",
    "labelsAndPreds = labeled_point.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(labeled_point.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra fun with PySpark: Calculate Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = spark_eurusd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate number of 5 minutes from number of minutes:\n",
    "minutes = lambda i: i * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Window and WindowSpec (in this case we need a time frame, e.g. 40 5 minutes) with rangeBetween():\n",
    "windowSpec = Window.orderBy(F.col(\"Time\").cast('long')).rangeBetween(-minutes(40), 0)\n",
    "\n",
    "# Note the OVER clause added to AVG(), to define a windowing column.\n",
    "data1 = data1.withColumn('ma40', F.avg(\"Close\").over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+-------+-------+----+-----------+----------+------------------+\n",
      "|               Time|   Open|   High|    Low|  Close|body|higher_wick|lower_wick|              ma40|\n",
      "+-------------------+-------+-------+-------+-------+----+-----------+----------+------------------+\n",
      "|2010-01-03 17:45:00| 1.4312|1.43172| 1.4312|1.43172| 5.2|        5.2|       5.2|           1.43172|\n",
      "|2010-01-03 18:00:00|1.43172|1.43425|1.43105| 1.4311| 6.2|       25.3|       0.5|           1.43141|\n",
      "|2010-01-03 18:15:00| 1.4315|1.43155| 1.4313|1.43155| 0.5|        0.5|       2.5|1.4314566666666668|\n",
      "|2010-01-03 18:30:00|1.43175| 1.4324|1.43106|1.43106| 6.9|        6.5|       0.0|1.4313575000000003|\n",
      "|2010-01-03 18:45:00|1.43111|1.43157|1.43106|1.43157| 4.6|        4.6|       5.1|1.4314000000000002|\n",
      "+-------------------+-------+-------+-------+-------+----+-----------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show(5, truncate= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
